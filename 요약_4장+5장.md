## 4장·5장 요약 정리 (자연어 처리, 트랜스포머, 허깅페이스 활용)

### 1) 핵심 개요

- **자연어 처리(NLP)**: 인간 언어를 컴퓨터가 이해·처리하도록 하는 인공지능 분야. 텍스트 분류, 감성 분석, 질의응답, 요약, 번역 등 다양한 응용.
- **모델 진화**: 전통 기법 → 통계 기반 → 딥러닝(RNN/LSTM/GRU) → Transformer(병렬·어텐션) → 대규모 언어모델(LLM).
- **임베딩 발전**: 원-핫/TF-IDF → Word2Vec/FastText → 문맥 임베딩(ELMo/BERT/GPT) → LLM의 컨텍스트 임베딩.

### 2) 텍스트 전처리·임베딩 핵심

- **전처리**: 정제(특수문자/대소문자), 불용어 제거, 토크나이징, 형태소 분석(KoNLPy), 품사 태깅.
- **문서 표현**: TF, TDM, TF-IDF. TF-IDF는 문서 특이성이 높은 단어에 더 큰 가중치.
- **Word2Vec**: CBOW(주변→중심), Skip-gram(중심→주변). 윈도우 크기: 작게(문법/지역 의미), 크게(주제/전반 의미).
- **실무 포인트**: gensim으로 간단 실습, SciPy 버전 호환 주의(2024.07 기준).

### 3) RNN 계열과 한계, 트랜스포머 도입

- **RNN/LSTM/GRU**: 순차 의존성 학습, 번역·음성·시계열 등에서 사용. 양방향 RNN으로 문맥 보강.
- **한계**: 긴 문장 장기 의존성, 순차 계산으로 인한 병렬화의 어려움.
- **Transformer**: 어텐션으로 전 구간 문맥을 병렬 처리. 인코더/디코더 구조, Self-Attention으로 긴 의존성 처리.

### 4) 사전학습 언어모델(PLM)과 아키텍처

- **인코더형(NLU)**: BERT(MLM/NSP), RoBERTa(NSP 제거·대규모), DistilBERT(지식증류로 경량/속도↑, 성능≈97%), ALBERT(임베딩 분리·파라미터 공유), ELECTRA(판별자 학습으로 효율↑), XLM·XLM-R(다국어), DeBERTa(상대위치·콘텐츠 분리).
- **디코더형(NLG)**: GPT(자기회귀), GPT-2/3(확장·few-shot), GPT-Neo/J(오픈 대안), CTRL(제어 토큰으로 스타일 제어).
- **인코더-디코더형**: T5(텍스트-투-텍스트 통합), BART(복원 기반), M2M-100(다언어 번역), BigBird(희소 어텐션·긴 문맥).

### 5) LLM 개념과 장점

- **Large의 의미**: 대규모 학습 데이터 + 대규모 파라미터. 창발적 능력 발현.
- **장점**: 다목적 전이학습, 소량 데이터로 파인튜닝 가능(Zero/Few-shot), 성능 지속 개선.

### 6) 허깅페이스(Transformers) 실무 활용

- **Hub**: 모델/데이터셋/스페이스. 다양한 도메인(NLP·CV·오디오·멀티모달).
- **pipeline()**: 전처리→모델→후처리를 1-2줄로 실행. 태스크: text-classification, zero-shot, summarization, translation 등.
- **AutoClass**: `AutoTokenizer`, `AutoModel*` 계열로 체크포인트 직접 로드해 커스텀 파이프라인/파인튜닝.
- **Inference API**: 토큰으로 손쉬운 HTTP 추론(속도·모델 제한 있음). 프로덕션은 Endpoints 고려.
- **Transformers.js**: 브라우저/Node에서 ONNX 기반 추론, CDN으로 간단 사용.

### 7) IMDB 영화평 분류 실습 요지(4장)

- **전통 접근(RNN 실습)**: IMDB 50k, 상위 10k 단어 + <OOV> 처리, 임베딩(32차원)→RNN→Dense(softmax)로 이진 감성 분류.
- **허깅페이스 접근**:
  - `distilbert-base-uncased`류로 시퀀스 분류 파인튜닝.
  - `TrainingArguments/Trainer`로 학습·평가·저장.
  - CPU 3epoch 약 10시간, RTX3090Ti 약 13분. 예시 성능: eval_loss≈0.27.
  - 저장/로드: `save_pretrained` / `from_pretrained`.
  - 추론: `tokenizer(text, return_tensors="pt")` → `model(**inputs)` → `argmax(logits)`.

### 8) 실무 체크리스트

- **데이터**: 토크나이저 일관성, <PAD>/<UNK>/<OOV> 인덱스 규약 확인.
- **학습**: 배치·학습률·에폭, 가중치감쇠, 평가 주기 설정. 가속기 설치(`accelerate`).
- **환경**: GPU 권장, CPU 시 학습시간 크다. SciPy/gensim 등 버전 충돌 주의.
- **배포**: 모델 저장·재현성(시드/버전) 관리, Inference API/Endpoints/ONNX 고려.

### 9) 한눈에 정리

- **핵심 메시지**: RNN에서 Transformer로의 전환이 NLP의 도약을 견인. 허깅페이스는 사전학습 모델 재사용·파인튜닝·배포를 표준화해 실무 생산성을 대폭 향상.
- **학습 경로**: 전처리/임베딩 기본기 → RNN 개념 이해 → Transformer/PLM 구조 → 허깅페이스 파이프라인·AutoClass → IMDB 감성 분류 파인튜닝 실습.

---

- **IMDB 특수 토큰 인덱스(4장 실습 기준)**:

  - 0: <PAD>, 1: <START>, 2: <OOV>, 3: <UNUSED>
  - 상위 빈도 단어 약 9,996개 + 특수 토큰으로 총 10k 단어 사용.

- **ChatGPT(Transformer) 임베딩 요점**:

  - 토큰 단위(BPE) 임베딩 행렬: [vocab_size, embedding_dim], 학습 가능 파라미터.
  - 컨텍스트 반영은 Self-Attention 층에서 수행되어 문맥적 임베딩으로 변환.

- **한국어 모델 활용 예**:

  - `beomi/kcbert-base`, `skt/kobert-base-v1` 등 한국어 토크나이저/모델 로드 후 토큰화·파인튜닝 가능.
  - 한국어 문장에서는 형태소/서브워드 기반 토크나이징 선택이 중요.

- **Inference API 사용시 유의**:

  - 일부 모델은 API 미제공. 모델 페이지의 Deploy → Inference API 여부 확인.
  - 무료 플랜은 속도/모델 제한. 프로덕션은 Inference Endpoints 권장.

- **추천 실습 순서**:
  1. 전처리·TF-IDF/Word2Vec 체험 → 2) RNN/양방향 RNN 실습 → 3) `pipeline()`으로 감성분류 빠른 검증 → 4) `Auto*` + `Trainer`로 DistilBERT 파인튜닝 → 5) 저장/불러오기/평가/배포(Endpoints/ONNX) 체인 구성.
